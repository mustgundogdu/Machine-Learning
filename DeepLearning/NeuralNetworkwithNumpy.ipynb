{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da6be883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Epoch Number: 0 / Squared Loss: 0.445089777569008\n",
      "====================\n",
      "Epoch Number: 50 / Squared Loss: 0.41136347900360615\n",
      "====================\n",
      "Epoch Number: 100 / Squared Loss: 0.3517339874215168\n",
      "====================\n",
      "Epoch Number: 150 / Squared Loss: 0.30946485769151544\n",
      "====================\n",
      "Epoch Number: 200 / Squared Loss: 0.2841673058494477\n",
      "====================\n",
      "Epoch Number: 250 / Squared Loss: 0.2691688005340343\n",
      "====================\n",
      "Epoch Number: 300 / Squared Loss: 0.2611176525516866\n",
      "====================\n",
      "Epoch Number: 350 / Squared Loss: 0.25683305311118787\n",
      "====================\n",
      "Epoch Number: 400 / Squared Loss: 0.2544611351940302\n",
      "====================\n",
      "Epoch Number: 450 / Squared Loss: 0.25307336420293697\n",
      "********************\n",
      "Input Value: [0. 0.]\n",
      "Predicted Value: [0.]\n",
      "Actual Value: [0.]\n",
      "********************\n",
      "Input Value: [0. 1.]\n",
      "Predicted Value: [0.]\n",
      "Actual Value: [1.]\n",
      "********************\n",
      "Input Value: [1. 0.]\n",
      "Predicted Value: [0.93385164]\n",
      "Actual Value: [1.]\n",
      "********************\n",
      "Input Value: [1. 1.]\n",
      "Predicted Value: [0.06696911]\n",
      "Actual Value: [0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNet:\n",
    "    \"\"\" Neural Network Create and Propagation Class \"\"\"\n",
    "    \n",
    "    ## Activation Functions sigmoid , relu and tanh\n",
    "    \n",
    "    # Sigmoid f(x) = 1 / 1 + e**-x\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    # Dervative sigmoid\n",
    "    @staticmethod\n",
    "    def dervativeSigmoid(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    # Hyperbolic Tangent (htan) f(x) = e**x - e**-x / e**x + e**-x\n",
    "    @staticmethod\n",
    "    def htan(x):\n",
    "        return np.tanh(x)\n",
    "    #Dervative tanh\n",
    "    @staticmethod\n",
    "    def derivativeHtan(x):\n",
    "        return 1.0 - x ** 2\n",
    "    \n",
    "    # Relu f(x) = max(0, x)\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    # Dervative Relu\n",
    "    @staticmethod\n",
    "    def derivativeRelu(x):\n",
    "        return 1 * (x > 0)\n",
    "     \n",
    "    def __init__(self, lr):\n",
    "        # Learning Rate\n",
    "        self.lr = lr\n",
    "        self.output = None\n",
    "        \n",
    "        ## Simple Neural Network 2 x 2 x 1\n",
    "        \n",
    "        # Weights | input - hidden (4), hidden - output (2) |\n",
    "        self.weights = [\n",
    "            # input - hidden\n",
    "            np.random.uniform(low=-0.2, high=0.2, size=(2, 2)),\n",
    "            # hidden - output\n",
    "            np.random.uniform(low=-2, high=2, size=(2,1))\n",
    "        ]\n",
    "        \n",
    "    def activationFunc(self, activationType, x):\n",
    "        if activationType == 'sigmoid':\n",
    "            return self.sigmoid(x)\n",
    "        if activationType == 'tanh':\n",
    "            return self.htan(x)\n",
    "        if activationType == 'relu':\n",
    "            return self.relu(x)\n",
    "        \n",
    "    def activationDerivate(self, activationType, x):\n",
    "        if activationType == 'sigmoid':\n",
    "            return self.dervativeSigmoid(x)\n",
    "        if activationType == 'tanh':\n",
    "            return self.derivativeHtan(x)\n",
    "        if activationType == 'relu':\n",
    "            return self.derivativeRelu(x)\n",
    "        \n",
    "    ## Feed Forward -> \n",
    "    def feedForwardPass(self, xVal):\n",
    "        # input -> hidden -> output\n",
    "        inputLayer = xVal\n",
    "        hiddenLayer = self.activationFunc('relu', np.dot(inputLayer, self.weights[0]))\n",
    "        outputLayer = self.activationFunc('relu', np.dot(hiddenLayer, self.weights[1]))\n",
    "        \n",
    "        # Create list on layers\n",
    "        self.layers = [\n",
    "            inputLayer,\n",
    "            hiddenLayer,\n",
    "            outputLayer\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        # Output Layer Value\n",
    "        return self.layers[2]\n",
    "    \n",
    "    ## Back Propagation <-\n",
    "    def backwardPropagation(self, targetOutput, actualVal):\n",
    "        # \n",
    "        delta = (targetOutput - actualVal)\n",
    "        \n",
    "        ## Chain Rule\n",
    "        for backward in range(2, 0, -1):\n",
    "            # < -\n",
    "            errDelta = delta * self.activationDerivate('sigmoid', self.layers[backward])\n",
    "            # Update weights with gradient\n",
    "            self.weights[backward - 1] += self.lr * np.dot(self.layers[backward - 1].T, errDelta)\n",
    "            \n",
    "            # Propagate using updated Weights and set new value On Neurons\n",
    "            delta = np.dot(errDelta, self.weights[backward - 1].T)\n",
    "            \n",
    "        \n",
    "    ## Training data using Propagation\n",
    "    def training(self, xVal, target):\n",
    "        self.output = self.feedForwardPass(xVal)\n",
    "        self.backwardPropagation(target, self.output)\n",
    "  \n",
    "    def predictVal(self, xVal):\n",
    "        return self.feedForwardPass(xVal)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Testing(NeuralNet):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Input Values\n",
    "        self.X = np.array(([0,0], [0,1], [1,0], [1,1]), dtype=float)\n",
    "        # Target Real Values\n",
    "        self.y = np.array(([0], [1], [1], [0]), dtype=float)\n",
    "        \n",
    "    def neuralNet(self):\n",
    "        # \n",
    "        nn = NeuralNet(lr=0.1)\n",
    "        epoch = 500\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            nn.training(self.X, self.y)\n",
    "            ten = epoch // 10\n",
    "            if i % ten == 0:\n",
    "                print(\"=\"*20)\n",
    "                print(\"Epoch Number: \" + str(i) + \" / \" + \"Squared Loss: \" + str(np.mean(np.square(self.y - nn.output))))\n",
    " \n",
    "        for inp in range(len(self.X)):\n",
    "            print(\"*\"*20)\n",
    "            print(\"Input Value: \"+str(self.X[inp]))\n",
    "            print(\"Predicted Value: \"+str(nn.predictVal(self.X[inp])))\n",
    "            print(\"Actual Value: \"+str(self.y[inp]))\n",
    "            \n",
    "            \n",
    "\n",
    "t = Testing()\n",
    "t.neuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c3a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
